{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04cc490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khanfarabi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the option: For the hotel review data : hotel : For the movie review data : movie\n",
      "hotel\n",
      "Enter the input phrase for hotel\n",
      "food\n",
      "Hotel Review\n",
      "['oosterpark ', 'outdoor ', 'interior ', 'helpful ', 'friendly ', 'lovely ', 'comfortable ', 'staff ', 'practical ', 'comfy ', 'park ', 'located ', 'welcoming ', 'modern ', 'proud ', 'spacious ', 'good ', 'loved ', 'overlooks ', 'grand ']\n",
      "55.57188677787781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-3b6a651e9276>:216: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  wrdvec[vb]=model[vb]\n",
      "<ipython-input-7-3b6a651e9276>:218: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  phrase_in[input_text1]=model[input_text1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport sys\\nL=list(sys.argv[1:])\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading Data\n",
    "import time\n",
    "start = time.time()\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "sto = set(stopwords.words(\"english\"))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords=[]\n",
    "for k in sto:\n",
    "    stopwords.append(k)\n",
    "\n",
    "flags = (re.UNICODE if sys.version < '3' and type(text) is unicode else 0)\n",
    "\n",
    "class data_preprocessing:\n",
    "    @classmethod\n",
    "    def hotel_data(cls):\n",
    "                    hotel=pd.read_csv(\"Hotel_Reviews.csv\")\n",
    "                    revid=0\n",
    "                    Words={}\n",
    "\n",
    "                    cn=0\n",
    "                    cn1=0\n",
    "                    for k in hotel['Negative_Review']:\n",
    "                        keep=[]\n",
    "                        if cn<500:\n",
    "                                #print(k)\n",
    "                                for word in re.findall(r\"\\w[\\w']*[^0-9]\", k, flags=flags):\n",
    "                                    if word.isdigit() or len(word)==1:\n",
    "                                        continue\n",
    "                                    word_lower = word.lower()\n",
    "                                    if word_lower in stopwords:\n",
    "                                            continue\n",
    "                                    if not any(c.isdigit() for c in word_lower) and \"'\" not in word_lower:\n",
    "                                        if word_lower.isdigit():\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                             if len(word_lower)>=4:\n",
    "                                                    keep.append(word_lower)\n",
    "                                if len(keep)>=10 and len(keep)<=40:\n",
    "                                        Words[revid]=keep\n",
    "                                        revid=revid+1\n",
    "\n",
    "                                cn=cn+1\n",
    "                    for k in hotel['Positive_Review']:\n",
    "                        keep=[]\n",
    "                        if cn1<500:\n",
    "                                #print(k)\n",
    "                                for word in re.findall(r\"\\w[\\w']*[^0-9]\", k, flags=flags):\n",
    "                                    if word.isdigit() or len(word)==1:\n",
    "                                        continue\n",
    "                                    word_lower = word.lower()\n",
    "                                    if word_lower in stopwords:\n",
    "                                            continue\n",
    "                                    word1 = wordnet_lemmatizer.lemmatize(word_lower, pos = \"n\")\n",
    "                                    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "                                    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "                                    if not any(c.isdigit() for c in  word3 ) and \"'\" not in word3:\n",
    "                                        if  word3 .isdigit():\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                             if len(word3)>3:\n",
    "                                                    keep.append(word_lower)\n",
    "                                if len(keep)>=10 and len(keep)<=40:\n",
    "                                        Words[revid]=keep\n",
    "                                        revid=revid+1\n",
    "\n",
    "                                cn1=cn1+1\n",
    "                                \n",
    "                    return Words\n",
    "    @classmethod\n",
    "    def movie_data(cls):\n",
    "                    f22=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "                    Words1={}\n",
    "                    rid=0\n",
    "                    cn2=0\n",
    "                    for k in f22['review']:\n",
    "                        keep=[]\n",
    "                        if cn2<1000:\n",
    "                                #print(k)\n",
    "                                for word in re.findall(r\"\\w[\\w']*[^0-9]\", k, flags=flags):\n",
    "                                    if word.isdigit() or len(word)==1:\n",
    "                                        continue\n",
    "                                    word_lower = word.lower()\n",
    "                                    if word_lower in stopwords:\n",
    "                                            continue\n",
    "                                    word1 = wordnet_lemmatizer.lemmatize(word_lower, pos = \"n\")\n",
    "                                    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "                                    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "                                    if not any(c.isdigit() for c in  word3 ) and \"'\" not in word3:\n",
    "                                        if  word3 .isdigit():\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                             if len(word3)>3:\n",
    "                                                    keep.append(word_lower)\n",
    "                                if len(keep)>=10 and len(keep)<=40:\n",
    "                                        Words1[rid]=keep\n",
    "                                        rid=rid+1\n",
    "\n",
    "                                cn2=cn2+1  \n",
    "                \n",
    "                    return Words1\n",
    "                \n",
    "                \n",
    "    @classmethod\n",
    "    def sent_generation(cls,Words):\n",
    "            sent=[]                         \n",
    "            for k in Words:\n",
    "                gh=[]\n",
    "                jj=str(k)\n",
    "                gh.append(jj)\n",
    "                for v in Words[k]:\n",
    "                    gh.append(v)\n",
    "                sent.append(gh)\n",
    "            return sent\n",
    "    @classmethod\n",
    "    #K-Means \n",
    "    def cluster(cls,Words):\n",
    "                    #cluster generation with k-means\n",
    "                    import sys\n",
    "                    from nltk.cluster import KMeansClusterer\n",
    "                    import nltk\n",
    "                    from sklearn import cluster\n",
    "                    from sklearn import metrics\n",
    "                    import gensim \n",
    "                    import operator\n",
    "                    from gensim.models import Word2Vec\n",
    "\n",
    "                    #Words=hotel_d\n",
    "\n",
    "                    model = Word2Vec(Words, min_count=1)\n",
    "\n",
    "                    X = model[model.wv.vocab]\n",
    "\n",
    "\n",
    "\n",
    "                    NUM_CLUSTERS=10\n",
    "                    kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,repeats=25)\n",
    "                    assigned_clusters1 = kclusterer.cluster(X,assign_clusters=True)\n",
    "                    #print (assigned_clusters)\n",
    "                    cluster={}\n",
    "                    words = list(model.wv.vocab)\n",
    "                    for i, word in enumerate(words):\n",
    "                      gh=[] \n",
    "                      gh1=[] \n",
    "                      gh2=[] \n",
    "                      if word.isdigit(): \n",
    "                        cluster[word]=assigned_clusters1[i]\n",
    "                        #print (word + \":\" + str(assigned_clusters[i]))\n",
    "                    cluster_final={}\n",
    "                    for j in range(NUM_CLUSTERS):\n",
    "                        gg=[]\n",
    "                        for tt in cluster:\n",
    "                            if int(cluster[tt])==int(j):\n",
    "                                if tt not in gg:\n",
    "                                    gg.append(tt)\n",
    "                        if len(gg)>0:\n",
    "                                    cluster_final[j]=gg\n",
    "                    cc=0\n",
    "                    final_clu={}\n",
    "                    for t in cluster_final:\n",
    "                        ghh=[]\n",
    "                        for k in cluster_final[t]:\n",
    "                            if int(k) in Words:\n",
    "                                   ghh.append(int(k))\n",
    "                        if len(ghh)>=2:\n",
    "                                final_clu[cc]=ghh\n",
    "                                cc=cc+1\n",
    "                    return final_clu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Find the similar words from the input data using user input\n",
    "class similar_phrase:\n",
    "    @classmethod\n",
    "    def same_words(cls,sent,m,Words):\n",
    "                        print(\"Enter the input phrase for \"+m)\n",
    "                        input_text1=input()\n",
    "                        vc=[input_text1]\n",
    "                        sent.append(vc)\n",
    "                        model = Word2Vec(sent, min_count=1,iter=100)\n",
    "                        #result = model.most_similar(positive=[input_text1],  topn=20)\n",
    "                        #for t in result:\n",
    "                            #if t[0].isdigit():\n",
    "                                #continue\n",
    "                          #  else:\n",
    "                                 # print(t)\n",
    "                        \n",
    "                        unique_words=[]\n",
    "                        ss=[]\n",
    "                        for t in Words:\n",
    "                            for kk in Words[t]:\n",
    "                                if kk not in ss:\n",
    "                                    ss.append(kk)\n",
    "                        s=set(ss)\n",
    "                        for v in s:\n",
    "                            unique_words.append(v)\n",
    "                        wrdvec={}\n",
    "                        for vb in unique_words:\n",
    "                            wrdvec[vb]=model[vb]\n",
    "                        phrase_in={}\n",
    "                        phrase_in[input_text1]=model[input_text1]\n",
    "\n",
    "                        wrd_sim=[]\n",
    "                        dc={}\n",
    "                        for bb in unique_words:\n",
    "                            vc1=wrdvec[bb]\n",
    "                            vc2=phrase_in[input_text1]\n",
    "                            #sm=1 - spatial.distance.cosine(vc1, vc2)\n",
    "                            sm1=dot(vc1,vc2)/(norm(vc1)*norm(vc2))\n",
    "                            dc[bb]=sm1\n",
    "                        import operator\n",
    "                        sorted_sim_map1 = sorted(dc.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                        cv=0\n",
    "                        for vc in  sorted_sim_map1:\n",
    "                            if cv<20:\n",
    "                                wrd_sim.append(vc[0])\n",
    "                                cv=cv+1\n",
    "                        #print(wrd_sim)\n",
    "                        return wrd_sim\n",
    "    @classmethod\n",
    "    def same_words_cl(cls,sent,m,Words1,cl):\n",
    "                        print(\"Enter the input phrase for \"+m)\n",
    "                        input_text1=input()\n",
    "                        vc=[input_text1]\n",
    "                        Words={}\n",
    "                        for t in Words1:\n",
    "                            if str(t) in cl or int(t) in cl:\n",
    "                                Words[t]=Words1[t]\n",
    "                        sent=[]                         \n",
    "                        for k in Words:\n",
    "                            gh=[]\n",
    "                            jj=str(k)\n",
    "                            gh.append(jj)\n",
    "                            for v in Words[k]:\n",
    "                                gh.append(v)\n",
    "                            sent.append(gh)\n",
    "                        sent.append(vc)\n",
    "                        model = Word2Vec(sent, min_count=1,iter=100)\n",
    "                        #result = model.most_similar(positive=[input_text1],  topn=20)\n",
    "                        #for t in result:\n",
    "                            #if t[0].isdigit():\n",
    "                                #continue\n",
    "                          #  else:\n",
    "                                 # print(t)\n",
    "                        unique_words=[]\n",
    "                        ss=[]\n",
    "                        for t in Words:\n",
    "                            for kk in Words[t]:\n",
    "                                if kk not in ss:\n",
    "                                    ss.append(kk)\n",
    "                        s=set(ss)\n",
    "                        for v in s:\n",
    "                            unique_words.append(v)\n",
    "                        wrdvec={}\n",
    "                        for vb in unique_words:\n",
    "                            wrdvec[vb]=model[vb]\n",
    "                        phrase_in={}\n",
    "                        phrase_in[input_text1]=model[input_text1]\n",
    "\n",
    "                        wrd_sim=[]\n",
    "                        dc={}\n",
    "                        for bb in unique_words:\n",
    "                            vc1=wrdvec[bb]\n",
    "                            vc2=phrase_in[input_text1]\n",
    "                            #sm=1 - spatial.distance.cosine(vc1, vc2)\n",
    "                            sm1=dot(vc1,vc2)/(norm(vc1)*norm(vc2))\n",
    "                            dc[bb]=sm1\n",
    "                        import operator\n",
    "                        sorted_sim_map1 = sorted(dc.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                        cv=0\n",
    "                        for vc in  sorted_sim_map1:\n",
    "                            if cv<20:\n",
    "                                wrd_sim.append(vc[0])\n",
    "                                cv=cv+1\n",
    "                        #print(wrd_sim)\n",
    "                        return wrd_sim\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "                        #1 - spatial.distance.cosine(vector1, vector2)\n",
    "                        \n",
    "#mwords=similar_phrase.same_words(movie_sent,\"movie\",movie_d)\n",
    "#print(\"Movie Review\")\n",
    "#print(mwords)\n",
    "print(\"Enter the option: For the hotel review data : hotel : For the movie review data : movie\")\n",
    "LL=input()\n",
    "if LL=='hotel':\n",
    "        hotel_d=data_preprocessing.hotel_data()\n",
    "        hotel_sent=data_preprocessing.sent_generation(hotel_d)\n",
    "\n",
    "        #hotel_cluster=data_preprocessing.cluster(hotel_d)\n",
    "        #movie_cluster=data_preprocessing.cluster(movie_d)\n",
    "        hwords=similar_phrase.same_words(hotel_sent,\"hotel\",hotel_d)\n",
    "        print(\"Hotel Review\")\n",
    "        print(hwords)\n",
    "elif LL=='movie':\n",
    "        movie_d=data_preprocessing.movie_data()\n",
    "        movie_sent=data_preprocessing.sent_generation(movie_d)\n",
    "        mwords=similar_phrase.same_words(movie_sent,\"movie\",movie_d)\n",
    "        print(\"Movie Review\")\n",
    "        print(mwords)\n",
    "#for k in hotel_cluster:\n",
    "    #pass#hwords=similar_phrase.same_words_cl(hotel_sent,\"hotel\",hotel_d,hotel_cluster[k])\n",
    "   # pass#print(\"Hotel Review\"+\"cluster_\"+str(k))\n",
    "    #pass#print(hwords)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "'''\n",
    "\n",
    "import sys\n",
    "L=list(sys.argv[1:])\n",
    "\n",
    "'''\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a22ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81465798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c046d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b1b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119250d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keyword Extraxt Using BERT\n",
    "\n",
    "hotel_text=\"\"\n",
    "for k in hotel_d:\n",
    "    for j in hotel_d[k]:\n",
    "        hotel_text=hotel_text+str(j)+\" \"\n",
    "#print(hotel_text)\n",
    "\n",
    "movie_text=\"\"\n",
    "for k in movie_d:\n",
    "    for j in movie_d[k]:\n",
    "        movie_text=movie_text+str(j)+\" \"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c2a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keyword Extraxt Using BERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def keys(text):\n",
    "            n_gram_range = (1, 1)\n",
    "            stop_words = \"english\"\n",
    "\n",
    "            # Extract candidate words/phrases\n",
    "            count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])\n",
    "            candidates = count.get_feature_names()\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "\n",
    "            model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "            doc_embedding = model.encode([text])\n",
    "            candidate_embeddings = model.encode(candidates)\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "            top_n =20\n",
    "            distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "            keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "            return keywords\n",
    "mkeys=keys(movie_text)\n",
    "hkeys=keys(hotel_text)\n",
    "print(\"movie keys\"+\"\\n\")\n",
    "print(mkeys)\n",
    "print(\"hotel keys\"+\"\\n\")\n",
    "print(hkeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3318af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4c9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257cea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e75019b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708385fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means Run 14\n",
    "def cluster(cls,Words):\n",
    "                    #cluster generation with k-means\n",
    "                    import sys\n",
    "                    from nltk.cluster import KMeansClusterer\n",
    "                    import nltk\n",
    "                    from sklearn import cluster\n",
    "                    from sklearn import metrics\n",
    "                    import gensim \n",
    "                    import operator\n",
    "                    from gensim.models import Word2Vec\n",
    "\n",
    "                    #Words=hotel_d\n",
    "\n",
    "                    model = Word2Vec(hotel_sent, min_count=1)\n",
    "\n",
    "                    X = model[model.wv.vocab]\n",
    "\n",
    "\n",
    "\n",
    "                    NUM_CLUSTERS=10\n",
    "                    kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,repeats=25)\n",
    "                    assigned_clusters1 = kclusterer.cluster(X,assign_clusters=True)\n",
    "                    #print (assigned_clusters)\n",
    "                    cluster={}\n",
    "                    words = list(model.wv.vocab)\n",
    "                    for i, word in enumerate(words):\n",
    "                      gh=[] \n",
    "                      gh1=[] \n",
    "                      gh2=[] \n",
    "                      if word.isdigit(): \n",
    "                        cluster[word]=assigned_clusters1[i]\n",
    "                        #print (word + \":\" + str(assigned_clusters[i]))\n",
    "                    cluster_final={}\n",
    "                    for j in range(NUM_CLUSTERS):\n",
    "                        gg=[]\n",
    "                        for tt in cluster:\n",
    "                            if int(cluster[tt])==int(j):\n",
    "                                if tt not in gg:\n",
    "                                    gg.append(tt)\n",
    "                        if len(gg)>0:\n",
    "                                    cluster_final[j]=gg\n",
    "                    cc=0\n",
    "                    final_clu={}\n",
    "                    for t in cluster_final:\n",
    "                        ghh=[]\n",
    "                        for k in cluster_final[t]:\n",
    "                            if int(k) in Words:\n",
    "                                   ghh.append(int(k))\n",
    "                        if len(ghh)>=2:\n",
    "                                final_clu[cc]=ghh\n",
    "                                cc=cc+1\n",
    "                    return final_clu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889ba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
