{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_Test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjMScZqJ3JPB",
        "outputId": "b90b73df-72c2-4598-b077-9d6a495f6980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Enter the option: For the hotel review data : hotel : For the movie review data : movie\n",
            "movie\n",
            "Enter the input phrase for movie\n",
            "star\n"
          ]
        }
      ],
      "source": [
        "#Reading Data\n",
        "import time\n",
        "start = time.time()\n",
        "import time\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from scipy import spatial\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "import random\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "sto = set(stopwords.words(\"english\"))\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stopwords=[]\n",
        "for k in sto:\n",
        "    stopwords.append(k)\n",
        "\n",
        "flags = (re.UNICODE if sys.version < '3' and type(text) is unicode else 0)\n",
        "\n",
        "class data_preprocessing:\n",
        "    @classmethod\n",
        "    def hotel_data(cls):\n",
        "                    hotel=pd.read_csv(\"Hotel_Reviews.csv\")\n",
        "                    revid=0\n",
        "                    Words={}\n",
        "\n",
        "                    cn=0\n",
        "                    cn1=0\n",
        "                    for k in hotel['Negative_Review']:\n",
        "                        keep=[]\n",
        "                        if cn<50000:\n",
        "                                #print(k)\n",
        "                                for word in re.findall(r\"\\w[\\w']*[^0-9]\", k, flags=flags):\n",
        "                                    if word.isdigit() or len(word)==1:\n",
        "                                        continue\n",
        "                                    word_lower = word.lower()\n",
        "                                    if word_lower in stopwords:\n",
        "                                            continue\n",
        "                                    word1 = wordnet_lemmatizer.lemmatize(word_lower, pos = \"n\")\n",
        "                                    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "                                    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
        "                                    if not any(c.isdigit() for c in  word3 ) and \"'\" not in word3:\n",
        "                                        if  word3 .isdigit():\n",
        "                                            continue\n",
        "                                        else:\n",
        "                                             if len(word3)>=3:\n",
        "                                                    keep.append(word3)\n",
        "                                if len(keep)>=10:\n",
        "                                        Words[revid]=keep\n",
        "                                        revid=revid+1\n",
        "                                cn=cn+1\n",
        "                    for k in hotel['Positive_Review']:\n",
        "                        keep=[]\n",
        "                        if cn1<50000:\n",
        "                                #print(k)\n",
        "                                for word in re.findall(r\"\\w[\\w']*[^0-9]\", k, flags=flags):\n",
        "                                    if word.isdigit() or len(word)==1:\n",
        "                                        continue\n",
        "                                    word_lower = word.lower()\n",
        "                                    if word_lower in stopwords:\n",
        "                                            continue\n",
        "                                    word1 = wordnet_lemmatizer.lemmatize(word_lower, pos = \"n\")\n",
        "                                    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "                                    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
        "                                    if not any(c.isdigit() for c in  word3 ) and \"'\" not in word3:\n",
        "                                        if  word3 .isdigit():\n",
        "                                            continue\n",
        "                                        else:\n",
        "                                             if len(word3)>=3:\n",
        "                                                    keep.append(word3)\n",
        "                                if len(keep)>=10:\n",
        "                                        Words[revid]=keep\n",
        "                                        revid=revid+1\n",
        "\n",
        "                                cn1=cn1+1\n",
        "                                \n",
        "                    return Words\n",
        "    @classmethod\n",
        "    def movie_data(cls):\n",
        "                    f22=pd.read_csv(\"IMDB Dataset.csv\")\n",
        "                    Words1={}\n",
        "                    rid=0\n",
        "                    cn2=0\n",
        "                    for k in f22['review']:\n",
        "                        keep=[]\n",
        "                        if cn2<5000:\n",
        "                                #print(k)\n",
        "                                for word in re.findall(r\"\\w[\\w']*[^0-9]\", k, flags=flags):\n",
        "                                    if word.isdigit() or len(word)==1:\n",
        "                                        continue\n",
        "                                    word_lower = word.lower()\n",
        "                                    if word_lower in stopwords:\n",
        "                                            continue\n",
        "                                    word1 = wordnet_lemmatizer.lemmatize(word_lower, pos = \"n\")\n",
        "                                    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "                                    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
        "                                    if not any(c.isdigit() for c in   word3 ) and \"'\" not in  word3:\n",
        "                                        if   word3.isdigit():\n",
        "                                            continue\n",
        "                                        else:\n",
        "                                             if len(word3)>3:\n",
        "                                                    keep.append(word3)\n",
        "                                if len(keep)>=25 :#and len(keep)<=40:\n",
        "                                        Words1[rid]=keep\n",
        "                                        rid=rid+1\n",
        "\n",
        "                                cn2=cn2+1  \n",
        "                \n",
        "                    return Words1\n",
        "                \n",
        "                \n",
        "    @classmethod\n",
        "    def sent_generation(cls,Words):\n",
        "            sent=[]                         \n",
        "            for k in Words:\n",
        "                gh=[]\n",
        "                jj=str(k)\n",
        "                gh.append(jj)\n",
        "                for v in Words[k]:\n",
        "                    gh.append(v)\n",
        "                sent.append(gh)\n",
        "            return sent\n",
        "    @classmethod\n",
        "    #K-Means \n",
        "    def cluster(cls,Words):\n",
        "                    #cluster generation with k-means\n",
        "                    import sys\n",
        "                    from nltk.cluster import KMeansClusterer\n",
        "                    import nltk\n",
        "                    from sklearn import cluster\n",
        "                    from sklearn import metrics\n",
        "                    import gensim \n",
        "                    import operator\n",
        "                    from gensim.models import Word2Vec\n",
        "\n",
        "                    #Words=hotel_d\n",
        "\n",
        "                    model = Word2Vec(Words, min_count=1)\n",
        "\n",
        "                    X = model[model.wv.vocab]\n",
        "\n",
        "\n",
        "\n",
        "                    NUM_CLUSTERS=10\n",
        "                    kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,repeats=25)\n",
        "                    assigned_clusters1 = kclusterer.cluster(X,assign_clusters=True)\n",
        "                    #print (assigned_clusters)\n",
        "                    cluster={}\n",
        "                    words = list(model.wv.vocab)\n",
        "                    for i, word in enumerate(words):\n",
        "                      gh=[] \n",
        "                      gh1=[] \n",
        "                      gh2=[] \n",
        "                      if word.isdigit(): \n",
        "                        cluster[word]=assigned_clusters1[i]\n",
        "                        #print (word + \":\" + str(assigned_clusters[i]))\n",
        "                    cluster_final={}\n",
        "                    for j in range(NUM_CLUSTERS):\n",
        "                        gg=[]\n",
        "                        for tt in cluster:\n",
        "                            if int(cluster[tt])==int(j):\n",
        "                                if tt not in gg:\n",
        "                                    gg.append(tt)\n",
        "                        if len(gg)>0:\n",
        "                                    cluster_final[j]=gg\n",
        "                    cc=0\n",
        "                    final_clu={}\n",
        "                    for t in cluster_final:\n",
        "                        ghh=[]\n",
        "                        for k in cluster_final[t]:\n",
        "                            if int(k) in Words:\n",
        "                                   ghh.append(int(k))\n",
        "                        if len(ghh)>=2:\n",
        "                                final_clu[cc]=ghh\n",
        "                                cc=cc+1\n",
        "                    return final_clu\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Find the similar words from the input data using user input\n",
        "class similar_phrase:\n",
        "    @classmethod\n",
        "    def same_words(cls,sent,m,Words):\n",
        "                        print(\"Enter the input phrase for \"+m)\n",
        "                        input_text1=input()\n",
        "                        vc=[input_text1]\n",
        "                        sent.append(vc)\n",
        "                        model = Word2Vec(sent, min_count=1,iter=100)\n",
        "                        #result = model.most_similar(positive=[input_text1],  topn=20)\n",
        "                        #for t in result:\n",
        "                            #if t[0].isdigit():\n",
        "                                #continue\n",
        "                          #  else:\n",
        "                                 # print(t)\n",
        "                        \n",
        "                        unique_words=[]\n",
        "                        ss=[]\n",
        "                        for t in Words:\n",
        "                            for kk in Words[t]:\n",
        "                                if kk not in ss:\n",
        "                                    ss.append(kk)\n",
        "                        s=set(ss)\n",
        "                        for v in s:\n",
        "                            unique_words.append(v)\n",
        "                        wrdvec={}\n",
        "                        for vb in unique_words:\n",
        "                            wrdvec[vb]=model[vb]\n",
        "                        phrase_in={}\n",
        "                        phrase_in[input_text1]=model[input_text1]\n",
        "\n",
        "                        wrd_sim=[]\n",
        "                        dc={}\n",
        "                        for bb in unique_words:\n",
        "                            vc1=wrdvec[bb]\n",
        "                            vc2=phrase_in[input_text1]\n",
        "                            #sm=1 - spatial.distance.cosine(vc1, vc2)\n",
        "                            sm1=dot(vc1,vc2)/(norm(vc1)*norm(vc2))\n",
        "                            dc[bb]=sm1\n",
        "                        import operator\n",
        "                        sorted_sim_map1 = sorted(dc.items(), key=operator.itemgetter(1),reverse=True)\n",
        "                        cv=0\n",
        "                        for vc in  sorted_sim_map1:\n",
        "                          if len(vc[0])>=4:\n",
        "                              if cv<20:\n",
        "                                if vc[0] not in wrd_sim:\n",
        "                                  if vc[0].isalnum():\n",
        "                                      wrd_sim.append(vc[0])\n",
        "                                      cv=cv+1\n",
        "                        #print(wrd_sim)\n",
        "                        return wrd_sim\n",
        "    @classmethod\n",
        "    def same_words_cl(cls,sent,m,Words1,cl):\n",
        "                        print(\"Enter the input phrase for \"+m)\n",
        "                        input_text1=input()\n",
        "                        vc=[input_text1]\n",
        "                        Words={}\n",
        "                        for t in Words1:\n",
        "                            if str(t) in cl or int(t) in cl:\n",
        "                                Words[t]=Words1[t]\n",
        "                        sent=[]                         \n",
        "                        for k in Words:\n",
        "                            gh=[]\n",
        "                            jj=str(k)\n",
        "                            gh.append(jj)\n",
        "                            for v in Words[k]:\n",
        "                                gh.append(v)\n",
        "                            sent.append(gh)\n",
        "                        sent.append(vc)\n",
        "                        model = Word2Vec(sent, min_count=1,iter=100)\n",
        "                        #result = model.most_similar(positive=[input_text1],  topn=20)\n",
        "                        #for t in result:\n",
        "                            #if t[0].isdigit():\n",
        "                                #continue\n",
        "                          #  else:\n",
        "                                 # print(t)\n",
        "                        unique_words=[]\n",
        "                        ss=[]\n",
        "                        for t in Words:\n",
        "                            for kk in Words[t]:\n",
        "                                if kk not in ss:\n",
        "                                    ss.append(kk)\n",
        "                        s=set(ss)\n",
        "                        for v in s:\n",
        "                            unique_words.append(v)\n",
        "                        wrdvec={}\n",
        "                        for vb in unique_words:\n",
        "                            wrdvec[vb]=model[vb]\n",
        "                        phrase_in={}\n",
        "                        phrase_in[input_text1]=model[input_text1]\n",
        "\n",
        "                        wrd_sim=[]\n",
        "                        dc={}\n",
        "                        for bb in unique_words:\n",
        "                            vc1=wrdvec[bb]\n",
        "                            vc2=phrase_in[input_text1]\n",
        "                            #sm=1 - spatial.distance.cosine(vc1, vc2)\n",
        "                            sm1=dot(vc1,vc2)/(norm(vc1)*norm(vc2))\n",
        "                            dc[bb]=sm1\n",
        "                        import operator\n",
        "                        sorted_sim_map1 = sorted(dc.items(), key=operator.itemgetter(1),reverse=True)\n",
        "                        cv=0\n",
        "                        for vc in  sorted_sim_map1:\n",
        "                            if cv<20:\n",
        "                              if vc[0].isalnum():\n",
        "                                wrd_sim.append(vc[0])\n",
        "                                cv=cv+1\n",
        "                        #print(wrd_sim)\n",
        "                        return wrd_sim\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "                        #1 - spatial.distance.cosine(vector1, vector2)\n",
        "                        \n",
        "#mwords=similar_phrase.same_words(movie_sent,\"movie\",movie_d)\n",
        "#print(\"Movie Review\")\n",
        "#print(mwords)\n",
        "print(\"Enter the option: For the hotel review data : hotel : For the movie review data : movie\")\n",
        "LL=input()\n",
        "if LL=='hotel':\n",
        "        hotel_d=data_preprocessing.hotel_data()\n",
        "        hotel_sent=data_preprocessing.sent_generation(hotel_d)\n",
        "\n",
        "        #hotel_cluster=data_preprocessing.cluster(hotel_d)\n",
        "        #movie_cluster=data_preprocessing.cluster(movie_d)\n",
        "        hwords=similar_phrase.same_words(hotel_sent,\"hotel\",hotel_d)\n",
        "        print(\"Hotel Review\")\n",
        "        print(hwords)\n",
        "elif LL=='movie':\n",
        "        movie_d=data_preprocessing.movie_data()\n",
        "        movie_sent=data_preprocessing.sent_generation(movie_d)\n",
        "        mwords=similar_phrase.same_words(movie_sent,\"movie\",movie_d)\n",
        "        print(\"Movie Review\")\n",
        "        print(mwords)\n",
        "#for k in hotel_cluster:\n",
        "    #pass#hwords=similar_phrase.same_words_cl(hotel_sent,\"hotel\",hotel_d,hotel_cluster[k])\n",
        "   # pass#print(\"Hotel Review\"+\"cluster_\"+str(k))\n",
        "    #pass#print(hwords)\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)\n",
        "'''\n",
        "\n",
        "import sys\n",
        "L=list(sys.argv[1:])\n",
        "\n",
        "'''\n",
        "                          "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SE_KxaG-wIE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AWLbFXzqwIK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BC5JZw7VnsYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['food', 'pizzas ', 'breakfast',  'steak ', 'pasta ', 'cuisine ', 'bananas ', 'desserts ', 'hams ', 'pastries ']\n"
      ],
      "metadata": {
        "id": "nUGJI6jUnscN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8QIEbDb7nsfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nnILDJH3nsjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['poutily ', 'toyoji,', 'contradicts ', 'larter.', 'accident.', 'tale.', 'prime,', 'bio ', 'coma,', 'fault,', 'dream.', 'centre ', 'perfection,', 'hands,', 'reactions ', 'innocent,', 'hated ', 'ali ', 'desired.', 'hurt ']"
      ],
      "metadata": {
        "id": "DjVwTFbVOtr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['destiny ', 'malcolm ', 'revenge,', 'girl,', 'trip.', 'toyoji,', 'christie ', 'joy ', 'aggh!', 'crappy\"', 'spade ', 'fantastically ', 'adding ', 'conveying ', 'robs ', 'perpetually ', 'recognize.', 'singer.', 'laughs!', 'spade.']"
      ],
      "metadata": {
        "id": "zmJMSsGGOuQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['haha.', 'fairly ', 'unfolding ', 'witch,', 'perspicacious ', 'actor,', 'rescued.', 'carpet ', 'carter.', 'abroad ', 'fatally ', 'green ', 'thus ', 'writer.', 'sores ', 'banks.', 'agree.', 'oberon ', 'wong ', 'shorty,']\n"
      ],
      "metadata": {
        "id": "KuQCNbR3fPq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['kid)', 'deborah ', 'scarfe ', 'rex.', 'piles ', 'wandering,', 'ditmar,', 'thomas.', 'fernando ', 'angry,', 'greatest,', 'decision.', 'oedipus ', 'matches?', 'hearn ', 'rapaport ', 'aniston ', 'helgeland ', 'mrs ', 'scamp)']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpCZcu43haSf",
        "outputId": "7076b255-3d65-427f-adc4-f125af99169c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "337.4536814689636"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hotel key='bathroom'\n",
        "['bathroom', 'restock ', 'enabling ', 'litet ', 'detail', 'waterloo', 'utility ', 'britis ', 'conferences ', 'absent ', 'theatre', 'westiminster ', 'highly', 'sparse', 'granite ', 'coulis ', 'rummet ', 'creased ', 'personalen ', 'handrails ']\n"
      ],
      "metadata": {
        "id": "_hwPhaf6h9Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#['food', 'plate', 'soups ', 'breakfest ', 'parc ', 'tine ', 'liberty ', 'shower', 'episode ', 'morning ', 'cocoa ', 'adventure', 'steamy ', 'statue ', 'hights ', 'misfortune ', 'stew ', 'smalle ', 'day ', 'pouring ']\n"
      ],
      "metadata": {
        "id": "usL-JFSQh9Uo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MMYRf081h9YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bqqL6IDph9bh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}